{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IXEB6JYoheH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "from sklearn.impute import KNNImputer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('TSS_data.csv')\n",
        "\n",
        "# Impute NaN values for the entire dataset using KNN\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "data_imputed = pd.DataFrame(imputer.fit_transform(data), columns=data.columns)\n",
        "\n",
        "# Separate features and target variables\n",
        "X = data_imputed.iloc[:, :-3]  # Exclude the last three columns (additional targets)\n",
        "y_tss = data_imputed['TSS']\n",
        "y_chla = data_imputed['Chla']\n",
        "y_cdom = data_imputed['CDOM']\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "\n",
        "def create_mdn_model(units=64, num_components=3, learning_rate=0.001):\n",
        "    model = Sequential([\n",
        "        Dense(units, activation='relu', input_dim=X_scaled.shape[1]),\n",
        "        Dense(num_components * 3, activation='linear')  # 3 parameters per component\n",
        "    ])\n",
        "\n",
        "    def mdn_loss(y_true, y_pred):\n",
        "        # Extract mean, log_sigma, and alpha using indexing\n",
        "        num_params = num_components * 3\n",
        "        mean = y_pred[:, :num_components]\n",
        "        log_sigma = y_pred[:, num_components:2*num_components]\n",
        "        alpha = y_pred[:, 2*num_components:]\n",
        "\n",
        "        sigma = K.exp(log_sigma)\n",
        "\n",
        "        # Gaussian probability density function\n",
        "        pdf = K.exp(-0.5 * K.square((y_true - mean) / (sigma + 1e-8))) / (sigma + 1e-8) / np.sqrt(2 * np.pi)\n",
        "\n",
        "        # Mixture of Gaussians\n",
        "        loss = -K.log(K.sum(alpha * pdf, axis=1, keepdims=True) + 1e-8)\n",
        "        return loss\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss=mdn_loss)\n",
        "    return model\n",
        "\n",
        "# Perform 10-fold cross-validation with different hyperparameter combinations for TSS\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "param_combinations = [(32, 2, 0.001), (64, 2, 0.001), (128, 2, 0.001),\n",
        "                      (32, 3, 0.001), (64, 3, 0.001), (128, 3, 0.001),\n",
        "                      (32, 4, 0.001), (64, 4, 0.001), (128, 4, 0.001)]\n",
        "\n",
        "def cross_validate_and_save(X, y, target_name, param_combinations):\n",
        "    mape_scores = []\n",
        "\n",
        "    for fold, (train_index, test_index) in enumerate(kf.split(X)):\n",
        "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "        y_train = y.iloc[train_index]\n",
        "        y_test = y.iloc[test_index]\n",
        "\n",
        "        for units, num_components, learning_rate in param_combinations:\n",
        "            mdn_model = create_mdn_model(units=units, num_components=num_components, learning_rate=learning_rate)\n",
        "\n",
        "            # Implement early stopping\n",
        "            early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "            mdn_model.fit(\n",
        "                X_train, y_train,\n",
        "                epochs=100,\n",
        "                batch_size=32,\n",
        "                validation_data=(X_test, y_test),\n",
        "                callbacks=[early_stopping],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            y_pred = mdn_model.predict(X_test)\n",
        "\n",
        "            mape = mean_absolute_percentage_error(y_test, y_pred[:, :1].flatten())  # Use mean for prediction\n",
        "            mape_scores.append((units, num_components, learning_rate, mape))\n",
        "\n",
        "            # Save predicted values\n",
        "            predictions_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred[:, :1].flatten()})\n",
        "            predictions_df.to_csv(f'{target_name}_predictions_fold_{fold}.csv', index=False)\n",
        "\n",
        "    # Find the best hyperparameters based on the lowest average MAPE\n",
        "    best_hyperparameters = min(mape_scores, key=lambda x: x[3])\n",
        "    best_units, best_num_components, best_learning_rate, best_mape = best_hyperparameters\n",
        "\n",
        "    # Print the best hyperparameters and average MAPE\n",
        "    print(f'Best Hyperparameters for {target_name}: Units={best_units}, Num Components={best_num_components}, Learning Rate={best_learning_rate}')\n",
        "    print(f'Average MAPE for {target_name} across 10 folds: {best_mape}')\n",
        "\n",
        "    return best_hyperparameters, best_mape\n",
        "\n",
        "# Cross-validate and save models for each target variable\n",
        "best_params_tss, mape_tss = cross_validate_and_save(X_scaled, y_tss, 'TSS', param_combinations)\n",
        "best_params_chla, mape_chla = cross_validate_and_save(X_scaled, y_chla, 'Chla', param_combinations)\n",
        "best_params_cdom, mape_cdom = cross_validate_and_save(X_scaled, y_cdom, 'CDOM', param_combinations)"
      ]
    }
  ]
}